{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPLy5Nl8Q63/DmMJ7NffvD4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alouwyck/vives-deep-learning/blob/main/topic17/3_Intro_LangChain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p34cmLp6FrA6"
      },
      "source": [
        "<img src=\"data:image/png;base64,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\" align=\"right\" /><br>\n",
        "\n",
        "\n",
        "**DEEP LEARNING**<br>\n",
        "Academiejaar 2024-2025<br>\n",
        "Andy Louwyck\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **INTRODUCTION TO LANGCHAIN**"
      ],
      "metadata": {
        "id": "Gglft8L4DvZC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[LangChain](https://www.langchain.com/) is a framework designed to simplify the creation of applications using large language models (LLMs). As a language model integration framework, LangChain's use-cases largely overlap with those of language models in general, including document analysis and summarization, chatbots, and code analysis.\n",
        "\n",
        "This notebook introduces key LangChain concepts and demonstrates their application in building a simple LLM application and a basic chatbot. Before diving into the code, it is explained what LangChain is."
      ],
      "metadata": {
        "id": "r5sj8xjzEK7x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **What is LangChain?**"
      ],
      "metadata": {
        "id": "CmiUPlcEL1Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Langchain is a comprehensive library designed for creating complex interaction flows with LLMs. It enables the development of applications that interact with LLMs and other data sources, providing superior capabilities compared to simple single-prompt applications."
      ],
      "metadata": {
        "id": "HLbdaXJnfDAH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The key features of Langchain are:\n",
        "\n",
        "1. **Chains**: Chains are critical in LangChain for composing modular components into reusable pipelines. In fact, they are sequences of calls to components, which can include other chains. They are necessary for more complex applications that require chaining LLMs with each other or with other components. Langchain provides a chain interface for such applications and offers different types of foundational chains.\n",
        "\n",
        "2. **Agents**: Agents are a key feature in LangChain for creating systems that interact dynamically with users and environments over time. An agent is an autonomous software entity that is capable of taking actions to accomplish goals and tasks. They are entities that decide which sequence of actions to take based on user input. They use an LLM as a reasoning engine to determine these actions. Agents have access to a suite of tools and memory, allowing them to interact with the world and remember the context of the conversation.\n",
        "\n",
        "3. **Memory**: In LangChain, memory refers to the persisting state between executions of a chain or agent. It allows agents to remember the context of the conversation. Langchain offers different types of memory, each with its own use cases and trade-offs. Each type of memory serves a specific purpose, from storing the immediate history of a conversation to extracting and summarizing entities from the conversation.\n",
        "\n",
        "4. **Tools**: Tools provide modular interfaces for agents to integrate external services like databases and APIs. Toolkits group tools that share resources. Tools can be combined with models to extend\n",
        "their capability. They perform specific duties and can be generic utilities, other chains, or even other agents. They allow the LLM to interact with the outside world and can be customized to perform any operation, not just predefined ones. LangChain offers tools like document loaders, indexes, and vector stores, which facilitate the retrieval and storage of data for augmenting data retrieval in LLMs.\n"
      ],
      "metadata": {
        "id": "cMzDEeN3cje6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chains, agents, memory, and tools enable the creation of sophisticated LLM applications that go beyond basic API calls to a single LLM. Langchain also provides a *callback system* that allows users to hook into various stages of the LLM application, useful for tasks like logging, monitoring, and streaming. The library is highly customizable, allowing users to create complex networks by leveraging these features. It's used in projects that require complex tasks like writing SQL queries, fetching data from a database, analyzing the data, and providing a response in the form of an answer. All these tasks happen automatically from start to finish, making Langchain a powerful tool for developing GenAI applications."
      ],
      "metadata": {
        "id": "_NpYP70hfGZS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup**"
      ],
      "metadata": {
        "id": "25498CxBxApb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First we need to setup our Google Colab environment, starting with installing LangChain:"
      ],
      "metadata": {
        "id": "24sCT2eoNbtK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q langchain"
      ],
      "metadata": {
        "id": "h8bWoQHVxAJO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll use the latest [mistral.ai](https://mistral.ai/) chat model:"
      ],
      "metadata": {
        "id": "1YQNrbRONhMh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-mistralai"
      ],
      "metadata": {
        "id": "mlgwbccMyv8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Of course, it is possible to use another LLM, such as one of OpenAI's [GPT](https://en.wikipedia.org/wiki/Generative_pre-trained_transformer), Google's [Gemini](https://en.wikipedia.org/wiki/Gemini_(language_model)), Anthropic's [Claude](https://en.wikipedia.org/wiki/Claude_(language_model)), or Meta's [LLama](https://en.wikipedia.org/wiki/Llama_(language_model)) models. To see how you can setup another chat model, check https://python.langchain.com/v0.2/docs/tutorials/llm_chain/#setup."
      ],
      "metadata": {
        "id": "9RXkLGjqPmx7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is highly recommended to add your API tokens for LangChain and Mistral to the Google Colab Secrets (left pain), and assign them to the following environment variables:"
      ],
      "metadata": {
        "id": "1IsewAxGN8WC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "for token in [\"LANGCHAIN_API_KEY\", \"MISTRAL_API_KEY\"]:\n",
        "    os.environ[token] = userdata.get(token)"
      ],
      "metadata": {
        "id": "Lk-OBGY8ylKw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We're ready now to explore LangChain!"
      ],
      "metadata": {
        "id": "b-Rc0GG7lDW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **LangChain Concepts**"
      ],
      "metadata": {
        "id": "AqiiAMw7w2EC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we discuss the following LangChain concepts and techniques:\n",
        "\n",
        "- Chat models\n",
        "- Messages\n",
        "- Output parsers\n",
        "- Prompt templates\n",
        "- Chat history\n",
        "- Streaming\n",
        "\n",
        "In the next sections, we'll apply these concepts and techniques to build a simple LLM application and basic chatbot.\n",
        "\n",
        "An detailed overview of LangChain architecture, concepts, and techniques can be found here: https://python.langchain.com/v0.2/docs/concepts."
      ],
      "metadata": {
        "id": "YcX6fsmyw9I_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat Models"
      ],
      "metadata": {
        "id": "A0pXNMCmQbDn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Chat models are language models that use a sequence of messages as inputs and return chat messages as outputs. They support the assignment of distinct roles to conversation messages, helping to distinguish messages from the AI, users, and instructions such as system messages (see section about Messages)."
      ],
      "metadata": {
        "id": "hApoaITXmkng"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As an example, let's import and instantiate the latest Mistral chat model:"
      ],
      "metadata": {
        "id": "MNrNijnoOG4K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
        "model"
      ],
      "metadata": {
        "id": "JEJ-xv2Dy9N4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "02650aff-0d6e-426f-a6b5-2b85cfb8f51f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatMistralAI(client=<httpx.Client object at 0x7d6edc059a80>, async_client=<httpx.AsyncClient object at 0x7d6edc0581f0>, mistral_api_key=SecretStr('**********'), model='mistral-large-latest')"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can simply pass a string to the `invoke` method to prompt the model. The result is an `AIMessage` object. The answer is stored in attribute `content`:"
      ],
      "metadata": {
        "id": "X71_0LQVOOsI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.invoke(\"what is 2 + 2?\")\n",
        "print(type(result))\n",
        "print()\n",
        "print(result.content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dtk34UjyzBu6",
        "outputId": "bded83b0-d7e0-488a-cec3-28f84d6c7f8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'langchain_core.messages.ai.AIMessage'>\n",
            "\n",
            "The answer to 2 + 2 is 4. This is a basic arithmetic operation in mathematics.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "When a string is passed in as input, it is converted to a `HumanMessage` object, and then passed to the underlying model."
      ],
      "metadata": {
        "id": "vYnlF98-njWo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Messages"
      ],
      "metadata": {
        "id": "SWK85QUaQqxF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `HumanMessage` object is constructed as follows:"
      ],
      "metadata": {
        "id": "a3GOgHGROyL2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "message = HumanMessage(content=\"what is 2 + 2?\")\n",
        "message"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NV6DJjzhyqn7",
        "outputId": "bf7e75b1-2224-48ca-a2c3-89d6ad5c85f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "HumanMessage(content='what is 2 + 2?')"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We pass the `HumanMessage` object to the `invoke` method. Actually, the method accepts a list of messages:"
      ],
      "metadata": {
        "id": "FyNFWUO-O9tH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.invoke([message]).content)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XsBskKgn0bLM",
        "outputId": "eefa0ca2-f080-4a83-cf3a-5613166e9895"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The answer to 2 + 2 is 4. This is a basic arithmetic operation where you're adding two numbers together.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "There's also `SystemMessage`, which tells the model how to behave:"
      ],
      "metadata": {
        "id": "qZNhR9LoQ4lE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage\n",
        "\n",
        "result = model.invoke([\n",
        "    SystemMessage('You have to explain each user question as if this user is a 6-year old child'),\n",
        "    HumanMessage('Who is Alan Turing?')\n",
        "])\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "lDx7bKKsREg9",
        "outputId": "cf3135bf-8d2e-49d9-857e-51fba022f211"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, imagine you're playing with a box of Lego blocks. Now, you have a set of instructions that tell you exactly how to build a cool spaceship, right? Alan Turing was a very smart man who came up with the idea of a machine that could follow instructions, just like you with your Lego, but for any kind of job, not just building toys. This machine is what we now call a computer.\\n\\nAlan Turing also did some very important work during a big war. He helped to figure out secret messages that the bad guys were sending. By understanding these messages, he helped the good guys to win the war. So, Alan Turing was like a superhero who used his brain to do amazing things!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the actual message is always stored in the `content` attribute."
      ],
      "metadata": {
        "id": "XT_LsT1HoGAz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Parsers"
      ],
      "metadata": {
        "id": "w_azEkxOS_Vq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In many cases it is more convenient to use an output parser to parse the `result` returned by the chat model. Here we're dealing with strings, so we instantiate a `StrOutputParser` and pass the `result` to its `invoke` method:"
      ],
      "metadata": {
        "id": "nk6RYKN_TCKe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "parser.invoke(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "id": "1ABVSFBlTQdf",
        "outputId": "b0d5a555-35ab-4b59-f35e-4aa814a17e44"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Sure, imagine you're playing with a box of Lego blocks. Now, you have a set of instructions that tell you exactly how to build a cool spaceship, right? Alan Turing was a very smart man who came up with the idea of a machine that could follow instructions, just like you with your Lego, but for any kind of job, not just building toys. This machine is what we now call a computer.\\n\\nAlan Turing also did some very important work during a big war. He helped to figure out secret messages that the bad guys were sending. By understanding these messages, he helped the good guys to win the war. So, Alan Turing was like a superhero who used his brain to do amazing things!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "In general, output parsers are responsible for taking the output of a model and transforming it to a more suitable format for downstream tasks, such as JSON, XML, YAML, CSV, or a Pandas DataFrame."
      ],
      "metadata": {
        "id": "ERyq1buBoy5E"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates"
      ],
      "metadata": {
        "id": "i7ZCyFbASXHz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that a *prompt* is natural language text describing the task that a chat model should perform.\n",
        "\n",
        "*Prompt templates* help to translate user input and parameters into instructions for a chat model. This can be used to guide a model's response, helping it understand the context, and generate relevant and coherent language-based output.\n",
        "\n",
        "Prompt templates take as input a dictionary, where each key represents a variable in the prompt template to fill in, and they output a `PromptValue`, which can be cast to a string or a list of messages."
      ],
      "metadata": {
        "id": "yQfnbTw-sDpi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `PromptTemplate` is used to format a single string. It is generally used for simpler inputs. For example, a common way to construct and use a PromptTemplate is as follows:"
      ],
      "metadata": {
        "id": "awf1jOMusiiA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import PromptTemplate\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
        "prompt = prompt_template.invoke({\"topic\": \"cats\"})\n",
        "prompt.to_string()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "-P-f-pw1f8l8",
        "outputId": "87c23c94-2db8-48d5-bcde-6c75ac81f21f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Tell me a joke about cats'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ay5rwTt3tARw",
        "outputId": "fcbbb3db-7b8f-413a-8a22-80a72909bc77"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='Tell me a joke about cats')]"
            ]
          },
          "metadata": {},
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The string as well as the list of messages can be passed to the chat model:"
      ],
      "metadata": {
        "id": "LWbqufTqtKZg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    parser.invoke(\n",
        "        model.invoke(prompt.to_messages())\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OfhD-XOVgHuH",
        "outputId": "e6696585-67da-4b6f-e3e9-c064169a833a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sure, here's a light-hearted cat joke for you:\n",
            "\n",
            "Why don't cats play poker in the jungle?\n",
            "\n",
            "Because there's too many cheetahs!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is, however, more convenient to use a `ChatPromptTemplate`. Using this template it is possible to format a list of messages. For example, a common way to construct and use a `ChatPromptTemplate` is as follows:"
      ],
      "metadata": {
        "id": "uMGn_yiCtqUr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"You are a helpful assistant\"),\n",
        "    (\"user\", \"Tell me a joke about {topic}\")\n",
        "])\n",
        "\n",
        "prompt_template.invoke({\"topic\": \"cats\"}).to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDOnfBsxheZi",
        "outputId": "a81d1ff7-b8fd-4570-f2ad-8f413a66efc4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='You are a helpful assistant'),\n",
              " HumanMessage(content='Tell me a joke about cats')]"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "    parser.invoke(\n",
        "        model.invoke(\n",
        "            prompt_template.invoke({\"topic\": \"AI\"}).to_messages()\n",
        "        )\n",
        "    )\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "728hos5_hj60",
        "outputId": "e25df32a-01ca-49af-ce7c-c372fe7615a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Why don't machines ever laugh at jokes?\n",
            "\n",
            "Because they have a \"hardware\" time processing \"humor\" algorithms! They're always too \"busy error-handling\" to appreciate a good punchline!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Another example:"
      ],
      "metadata": {
        "id": "CBFpXxlruQt_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages([\n",
        "    (\"system\", \"Answer the questions as if the user is {age} years old\"),\n",
        "    (\"user\", \"Tell me who {person} is\")\n",
        "]).invoke(dict(age=10, person='Geoffry Hinton'))\n",
        "prompt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6mJL0h5Kk_zt",
        "outputId": "a46632b4-4087-458a-85f7-40e438a5c90a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Answer the questions as if the user is 10 years old'), HumanMessage(content='Tell me who Geoffry Hinton is')])"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "parser.invoke(model.invoke(prompt.to_messages()))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "DyTaW1W-mCxM",
        "outputId": "9f7feffd-21e7-4a09-f7a2-9172be70e5b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Sure, I\\'d be happy to explain!\\n\\nGeoffrey Hinton is a very smart person, kind of like a superhero, but instead of fighting bad guys, he uses his brain to understand how our minds work. He\\'s a scientist who specializes in something called \"artificial intelligence\" or \"AI\" for short. This is a field where scientists try to make computers think and learn like humans do.\\n\\nJust like how you learn from your teachers and parents, Geoffrey Hinton helped create something called \"neural networks\" which help computers learn from data. He\\'s often called the \"Godfather of Deep Learning\" because he\\'s been a big part of making AI what it is today. But don\\'t worry, he doesn\\'t have a fancy cape or anything like that, he just uses his knowledge and ideas to help make computers smarter!'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recall that the process of structuring an instruction that can be interpreted and understood by a generative AI model is called *prompt engineering*."
      ],
      "metadata": {
        "id": "arDgKxkD3xID"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chat History"
      ],
      "metadata": {
        "id": "xMHWjID-ue21"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Most LLM applications have a conversational interface. An essential component of a conversation is being able to refer to information introduced earlier in the conversation. At bare minimum, a conversational system should be able to access some window of past messages directly.\n",
        "\n",
        "The concept of `ChatHistory` refers to a class in LangChain which can be used to wrap an arbitrary chain. This `ChatHistory` will keep track of inputs and outputs of the underlying chain, and append them as messages to a message database. Future interactions will then load those messages and pass them into the chain as part of the input.\n",
        "\n",
        "This concept will be applied in the chatbot example."
      ],
      "metadata": {
        "id": "lqW2oPurujWL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "I0OeTjvuuglV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A `ChatModel` implements a sync method called `stream` and an async variant called `astream`. These methods are designed to stream the final output in chunks, yielding each chunk as soon as it is available.\n",
        "\n",
        "This technique will be demonstrated in the chatbot example."
      ],
      "metadata": {
        "id": "DOx6N8Iju-Q4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build a Simple LLM Application**"
      ],
      "metadata": {
        "id": "dW4ovkl48d4X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In previous section, we've already built a few simple LLM applications to explain some of the key LangChain concepts and techniques. In this section, we build an LLM application that will translate text from English into another language.\n",
        "\n",
        "This is a relatively simple LLM application - it's just a single LLM call plus some prompting. What's new here is the use of LangChain Expression Language, or **LCEL**, a declarative way to chain LangChain components.\n",
        "\n",
        "The example is adopted from the following LangChain quickstart: https://python.langchain.com/v0.2/docs/tutorials/llm_chain/\n",
        "\n"
      ],
      "metadata": {
        "id": "-74Ecqe98jKh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "P8BMRur1As7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already setup our environment, so we may skip this part of the tutorial.\n",
        "\n",
        "If you want to trace the LLM application using *LangSmith*, then you need to set the following environment variable:"
      ],
      "metadata": {
        "id": "-emVqCvS6Mjv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ],
      "metadata": {
        "id": "ooTBQhsG9AlH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using Language Models"
      ],
      "metadata": {
        "id": "ONNbo1ucA4HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's re-import and re-instantiate the latest large MISTRAL model:"
      ],
      "metadata": {
        "id": "RLlB4TiC6Ajf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "\n",
        "model = ChatMistralAI(model=\"mistral-large-latest\")"
      ],
      "metadata": {
        "id": "g1b06FqH-WE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain supports many different language models that you can use interchangably. Check the tutorial to see how you can use other models such as GPT-4, Claude-3-Sonnet, or Gemini!"
      ],
      "metadata": {
        "id": "5qoRr27c64-S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first use the model directly. `ChatModel` instances are LangChain \"Runnables\", which implies they expose a standard interface for interacting with them. To just simply call the model, we can pass in a list of messages to the `invoke` method:"
      ],
      "metadata": {
        "id": "6whs_4e07i1z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(content=\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nu38rr9a-elU",
        "outputId": "119cc581-9813-47a1-e407-8fceb0d7a107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hello! In Italian, this is commonly translated as \"Ciao!\" Please note that, like in English, \"Ciao\" can be used both for greeting someone and saying goodbye.', response_metadata={'token_usage': {'prompt_tokens': 15, 'total_tokens': 54, 'completion_tokens': 39}, 'model': 'mistral-large-latest', 'finish_reason': 'stop'}, id='run-918759f5-0971-4215-b909-be5e51f68bae-0', usage_metadata={'input_tokens': 15, 'output_tokens': 39, 'total_tokens': 54})"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Output Parsers"
      ],
      "metadata": {
        "id": "V22nOKX7BRdQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the response from the model is an `AIMessage`. This contains a string response along with other metadata about the response. Oftentimes we may just want to work with the string response. We can parse out just this response by using a simple output parser.\n",
        "\n",
        "We first import the simple output parser. One way to use it is to use it by itself. For example, we could save the `result` of the language model call and then pass it to the `parser`:"
      ],
      "metadata": {
        "id": "BMhC2PKa752h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "parser = StrOutputParser()\n",
        "\n",
        "result = model.invoke(messages)\n",
        "\n",
        "print(parser.invoke(result))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d1FJjbbM_HId",
        "outputId": "149f7272-f4da-4cec-f41f-a9c574faf5f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! In Italian, this is typically translated as \"Ciao!\"\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "More commonly, we can \"chain\" the model with this output parser. This means this output parser will get called every time in this chain. This chain takes on the input type of the language model (string or list of message) and returns the output type of the output parser (string).\n",
        "\n",
        "We can easily create the chain using the `|` operator, which is used in LangChain to combine two elements together:"
      ],
      "metadata": {
        "id": "19WYDmQV8UEz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = model | parser\n",
        "print(chain.invoke(messages))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ii9UrtLg_mMB",
        "outputId": "61f2e6e6-8129-47b7-ab75-72df63a9c9b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! In Italian, this is typically translated as \"Ciao!\" Please note that, like in English, \"Ciao\" can be used both for greeting and saying goodbye.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates"
      ],
      "metadata": {
        "id": "7Azpz36HBYuF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now we are passing a list of messages directly into the language model. Where does this list of messages come from? Usually, it is constructed from a combination of user input and application logic. This application logic usually takes the raw user input and transforms it into a list of messages ready to pass to the language model. Common transformations include adding a system message or formatting a template with the user input.\n",
        "\n",
        "Prompt templates are a concept in LangChain designed to assist with this transformation. They take in raw user input and return data (a prompt) that is ready to pass into a language model."
      ],
      "metadata": {
        "id": "p5O9zSQ5Ifvb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's create a `ChatPromptTemplate` here. It will take in two user variables:\n",
        "\n",
        "- `'language'`: the language to translate text into;\n",
        "- `'text'`: the text to translate.\n",
        "\n",
        "First, we create a string that we will format to be the system message. Next, we create the `ChatPromptTemplate`, which is a combination of the `system_template` as well as a simpler template for where to put the text to be translated. The input to this prompt template is a dictionary. We can play around with this prompt template by itself to see what it does by itself:"
      ],
      "metadata": {
        "id": "QyXb0R3AJ3-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following into {language}:\"  # template for the system message\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")\n",
        "\n",
        "result = prompt_template.invoke({\"language\": \"italian\", \"text\": \"hi\"})\n",
        "\n",
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QsdUCyA6Br5Z",
        "outputId": "7c54f45b-7d5d-468e-b411-e699af491f56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following into italian:'), HumanMessage(content='hi')])"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it returns a `ChatPromptValue` that consists of two messages. If we want to access the messages directly we do:"
      ],
      "metadata": {
        "id": "OjD4ym9AKHmv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result.to_messages()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HEIrRsZ-CNnN",
        "outputId": "14cbde3e-bddc-45ba-e667-592ca9faa9ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following into italian:'),\n",
              " HumanMessage(content='hi')]"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Chaining together components with LCEL"
      ],
      "metadata": {
        "id": "MZ12Pe3vChlg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now combine this with the model and the output parser from above using the pipe (`|`) operator:"
      ],
      "metadata": {
        "id": "blRCEfKCCzJV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | model | parser\n",
        "\n",
        "print(chain.invoke({\"language\": \"italian\", \"text\": \"hi\"}))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Tc1M6ORClSv",
        "outputId": "e3adb752-05da-4f3a-95a2-f83f0ae97b69"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello! The translation of \"Hi\" into Italian is \"Ciao\".\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is a simple example of using LangChain Expression Language (LCEL) to chain together LangChain modules. There are several benefits to this approach, including optimized streaming and tracing support."
      ],
      "metadata": {
        "id": "zF4KV0tUKmZO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Serving with LangServe"
      ],
      "metadata": {
        "id": "WNIW7uLQKtoH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we've built an application, we can serve it. That's where LangServe comes in. LangServe helps developers deploy LangChain chains as a REST API. Check the LangChain tutorial to learn how this is done!"
      ],
      "metadata": {
        "id": "flDiC4wmK8P_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "4ngt9wE_LWIU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "That's it! In this quickstart you've learned how to create your first simple LLM application. You've learned how to work with language models, how to parse their outputs, how to create a prompt template, and chaining them with LCEL.\n",
        "\n",
        "This just scratches the surface of what you will want to learn to become a proficient AI Engineer. In the next section, we'll build a simple chatbot!"
      ],
      "metadata": {
        "id": "YnuAfH6yLYIa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Build a Chatbot**"
      ],
      "metadata": {
        "id": "aPmDmAp6hcNp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We'll go over an example of how to design and implement an LLM-powered chatbot. This chatbot will be able to have a conversation and remember previous interactions. The example is adopted from the following LangChain tutorial: https://python.langchain.com/v0.2/docs/tutorials/chatbot/\n",
        "\n",
        "Note that this chatbot that we build will only use the language model to have a conversation. There are several other related concepts that you may be looking for:\n",
        "\n",
        "- *Conversational RAG*: Enable a chatbot experience over an external source of data\n",
        "- *Agents*: Build a chatbot that can take actions\n",
        "\n",
        "This section will cover the basics which will be helpful for those two more advanced topics."
      ],
      "metadata": {
        "id": "Bd__7rIKMxGc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setup"
      ],
      "metadata": {
        "id": "9kTrC_D8hpty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We've already setup our environment, so we may skip this part of the tutorial."
      ],
      "metadata": {
        "id": "6pHdr-SziAFo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quickstart"
      ],
      "metadata": {
        "id": "h7bMvUAbhySF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First up, let's use the MISTRAL chat model by itself, as we did in previous sections:"
      ],
      "metadata": {
        "id": "zdHZdfHoNv9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_mistralai import ChatMistralAI\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "model = ChatMistralAI(model=\"mistral-large-latest\")\n",
        "result = model.invoke([HumanMessage(content=\"Hi! I'm Bob\")])\n",
        "result.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "B-hBJcIeiFOW",
        "outputId": "1ec0552e-717d-432f-d867-ce6f82528c53"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Bob! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model on its own does not have any concept of *state*. For example, if you ask a followup question:"
      ],
      "metadata": {
        "id": "dHUKqAy5PF4N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.invoke([HumanMessage(content=\"What's my name?\")]).content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "MJdg50eViR_w",
        "outputId": "48a6efa5-17b9-4e6f-d38b-e2185bcd04cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm an assistant and I don't have personal information about individuals unless it has been shared with me in the course of our conversation. I'm here to help answer your questions to the best of my ability. If you'd like to tell me your name, I'll be happy to use it during our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 83
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see that it doesn't take the previous conversation turn into context, and cannot answer the question. This makes for a terrible chatbot experience! To get around this, we need to pass the entire conversation history into the model. Let's see what happens when we do that:"
      ],
      "metadata": {
        "id": "2JI32kuvPPun"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import AIMessage\n",
        "\n",
        "model.invoke(\n",
        "    [\n",
        "        HumanMessage(content=\"Hi! I'm Bob\"),\n",
        "        AIMessage(content=\"Hello Bob! How can I assist you today?\"),\n",
        "        HumanMessage(content=\"What's my name?\"),\n",
        "    ]\n",
        ").content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qIv3awnBjGN4",
        "outputId": "e42b1eef-8971-4800-f4ab-76b28e11e219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on your previous message, your name is Bob.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And now we can see that we get a good response! This is the basic idea underpinning a chatbot's ability to interact conversationally. So how do we best implement this?"
      ],
      "metadata": {
        "id": "nFSBccMxPkwJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Message History"
      ],
      "metadata": {
        "id": "IV1g1rmUOFhP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can use a `MessageHistory` class to wrap our model and make it stateful. This will keep track of inputs and outputs of the model, and store them in some datastore. Future interactions will then load those messages and pass them into the chain as part of the input. Let's see how to use this!"
      ],
      "metadata": {
        "id": "1RTgLtrcPtv9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We import the relevant classes and set up our chain which wraps the model and adds in this message history. A key part here is the function we pass into as the `get_session_history`. This function is expected to take in a `session_id` and return a `MessageHistory` object. This `session_id` is used to distinguish between separate conversations, and should be passed in as part of the config when calling the new chain (we'll show how to do that)."
      ],
      "metadata": {
        "id": "N_OhpTjdP6VN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.chat_history import (\n",
        "    BaseChatMessageHistory,\n",
        "    InMemoryChatMessageHistory,\n",
        ")\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str) -> BaseChatMessageHistory:\n",
        "    if session_id not in store:\n",
        "        store[session_id] = InMemoryChatMessageHistory()\n",
        "    return store[session_id]\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(model, get_session_history)"
      ],
      "metadata": {
        "id": "CXUqWXkejqck"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We now need to create a `config` that we pass into the runnable every time. This `config` contains information that is not part of the input directly, but is still useful. In this case, we want to include a `session_id`. This should look like:"
      ],
      "metadata": {
        "id": "MMREQC1mQZSE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"abc2\"}}\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi! I'm Bob\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uFo-rBahj8JC",
        "outputId": "76dec262-1e0f-4b18-8b5d-3cf56989745d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Bob! It's nice to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's check if it remembers the user's name:"
      ],
      "metadata": {
        "id": "yqNHHKwdQvFD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "7OdInvilkDwQ",
        "outputId": "da1f005a-c625-45d9-cc5e-c3714c85372e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on our previous interaction, your name is Bob. Is there anything else you would like to know or discuss?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Great! Our chatbot now remembers things about us. If we change the `config` to reference a different `session_id`, we can see that it starts the conversation fresh:"
      ],
      "metadata": {
        "id": "1qEoz-NDQtzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"abc3\"}}  # new session id\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "W_LAfdgJkMRU",
        "outputId": "39811934-8f29-450b-a594-5206fb9cfba2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm an assistant and I don't have personal information about individuals unless it has been shared with me in the course of our conversation. If we have not spoken before, I would not know your name. If you would like to tell me your name, I can use it to address you in our conversation.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 88
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "However, we can always go back to the original conversation (since we are persisting it in a database):"
      ],
      "metadata": {
        "id": "l9Jb1xI7RIIP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"abc2\"}}  # go back to previous session\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "a9tb2WnskVEk",
        "outputId": "1a8045b0-09f5-4e37-821e-0e94575d3188"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"As I mentioned earlier, your name is Bob. If you have any other questions or topics you'd like to discuss, feel free to let me know!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 89
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is how we can support a chatbot having conversations with many users!"
      ],
      "metadata": {
        "id": "T2DokF0BROtf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Right now, all we've done is add a simple persistence layer around the model. We can start to make the more complicated and personalized by adding in a prompt template."
      ],
      "metadata": {
        "id": "qwHq-mO2RRKz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prompt Templates\n"
      ],
      "metadata": {
        "id": "SKqE67Bfko49"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we've already discussed, prompt templates help to turn raw user information into a format that the LLM can work with. In this case, the raw user input is just a message, which we are passing to the LLM.\n",
        "\n",
        "Let's now make that a bit more complicated. First, let's add in a system message with some custom instructions (but still taking messages as input). Next, we'll add in more input besides just the messages.\n",
        "\n",
        "Let's add in a `SystemMessage` first. To do this, we will create a `ChatPromptTemplate`. We will utilize `MessagesPlaceholder` to pass all the messages in."
      ],
      "metadata": {
        "id": "-Qy0OtxwRWL9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability.\"),  # system message\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),  # placeholder for user messages\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "wrHG9fj4lfMj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that this slightly changes the input type - rather than pass in a list of messages, we are now passing in a dictionary with a `'messages'` key where that contains a list of messages:"
      ],
      "metadata": {
        "id": "j73dPHCxSAIw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke({\"messages\": [HumanMessage(content=\"hi! I'm bob\")]})\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "1hNu9lBFlzo9",
        "outputId": "c428a382-2e84-4515-adc2-121cfe46515d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Bob! It's a pleasure to meet you. How can I assist you today?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can now wrap this in the same `MessagesHistory` object as before:"
      ],
      "metadata": {
        "id": "wRtREWX7SRIy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(chain, get_session_history)\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"abc5\"}}\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"Hi! I'm Jim\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "6x8JmtVWmDJX",
        "outputId": "0e65a159-2cf7-43b3-9c24-7f7742863950"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Hello Jim! I'm here to help. Please feel free to ask me any questions you have, and I'll do my best to provide you with accurate and helpful information.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Checking whether it remembers the user's name:"
      ],
      "metadata": {
        "id": "-Eb4xMkMSkTS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    [HumanMessage(content=\"What's my name?\")],\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "49hwdyDQmM3p",
        "outputId": "f84d0c69-e4f8-45b2-9180-7ecbfecb664d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Based on the information given in your previous message, your name is Jim.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 93
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Awesome! Let's now make our prompt a little bit more complicated. Let's assume that the prompt template now looks something like this:"
      ],
      "metadata": {
        "id": "hIVAPV9pScdy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\"),\n",
        "      MessagesPlaceholder(variable_name=\"messages\")]\n",
        ")\n",
        "\n",
        "chain = prompt | model"
      ],
      "metadata": {
        "id": "IQC59o6gmT3O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we have added a new `'language'` input to the prompt. We can now invoke the chain and pass in a language of our choice:"
      ],
      "metadata": {
        "id": "jUJLznNoS34K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"hi! I'm bob\")], \"language\": \"Spanish\"}\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "tx7CgQ7AmjIT",
        "outputId": "103dac87-f89e-4478-a21e-2988caa38290"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hola! Es un placer conocerte, Bob. En qu puedo ayudarte hoy?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now wrap this more complicated chain again in a `MessageHistory` class. This time, because there are multiple keys in the input, we need to specify the correct key `'messages'` to use to save the chat history:"
      ],
      "metadata": {
        "id": "kWDp4n4gTIZK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")"
      ],
      "metadata": {
        "id": "VpwFTH0PnDZM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Invoking the chain:"
      ],
      "metadata": {
        "id": "O37JXjxTT9XS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"hi! I'm todd\")], \"language\": \"Spanish\"},\n",
        "    config=config,\n",
        ")\n",
        "\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "noLk0768nF1l",
        "outputId": "813e7d67-a04c-433d-96f4-5d3ff5cb56cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hola, soy un asistente til. Responder a todas las preguntas lo mejor que pueda en espaol.\\n\\nHola, Todd! En qu puedo ayudarte hoy?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 97
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "And checking whether it remembers the user's name:"
      ],
      "metadata": {
        "id": "Jp6iBs_EUDmI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"whats my name?\")], \"language\": \"Spanish\"},\n",
        "    config=config,\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "XQZPOB9PnOX_",
        "outputId": "03bcd23b-c12f-4fde-b829-d2e1349eadfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on the information you provided, your name is Todd.\\n\\nCmo puedo ayudarte, Todd? If you have any questions or need help with something, feel free to ask. I'm here to help you.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 98
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Managing Conversation History"
      ],
      "metadata": {
        "id": "8O2Dfnqwn7we"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
        "\n",
        "*Importantly*, you will want to do this BEFORE the prompt template but AFTER you load previous messages from `MessageHistory`!\n",
        "\n",
        "We can do this by adding a simple step in front of the prompt that modifies the messages key appropriately, and then wrap that new chain in the `MessageHistory` class."
      ],
      "metadata": {
        "id": "wNPIWrPiVpXu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "LangChain comes with a few built-in helpers for managing a list of messages. In this case we'll use the `trim_messages` helper to reduce how many messages we're sending to the model. The trimmer allows us to specify how many tokens we want to keep, along with other parameters like if we want to always keep the system message and whether to allow partial messages:"
      ],
      "metadata": {
        "id": "sKogAMAAVzrS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=50,  # setting to 65 keeps all messages...\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"having fun?\"),\n",
        "    AIMessage(content=\"yes!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Deq4166ApQou",
        "outputId": "702ccf96-dbe5-4d49-884d-693eaf559ce1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\"),\n",
              " HumanMessage(content='I like vanilla ice cream'),\n",
              " AIMessage(content='nice'),\n",
              " HumanMessage(content='whats 2 + 2'),\n",
              " AIMessage(content='4'),\n",
              " HumanMessage(content='thanks'),\n",
              " AIMessage(content='no problem!'),\n",
              " HumanMessage(content='having fun?'),\n",
              " AIMessage(content='yes!')]"
            ]
          },
          "metadata": {},
          "execution_count": 101
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note that we need to set environment variable `HF_TOKEN` containing our Hugging Face API key!"
      ],
      "metadata": {
        "id": "89zTdWX_WeXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To use it in our chain, we just need to run the trimmer before we pass the messages input to our prompt. Now if we try asking the model our name, it won't know it since we trimmed that part of the chat history:"
      ],
      "metadata": {
        "id": "xFM-rltCW9pP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)  # trimmer comes AFTER the messages\n",
        "    | prompt                                                               # and BEFORE the prompt template\n",
        "    | model\n",
        ")\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"what's my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "uKukP_2oqJOQ",
        "outputId": "7c83af4a-c17d-475a-9764-c956bd707d75"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I don't have the ability to know your name unless you tell me. How can I assist you further?\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 102
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "But if we ask about information that is within the last few messages, it remembers:"
      ],
      "metadata": {
        "id": "OWPpZ9zbXloK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = chain.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"what math problem did i ask\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "qCForxpCq1Wy",
        "outputId": "7742780e-7207-4578-d7f2-550ca255995b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'You asked \"whats 2 + 2\".'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 103
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now wrap this in the `MessageHistory`:"
      ],
      "metadata": {
        "id": "pW5orbg-XsEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"session_id\": \"abc20\"}}\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": messages + [HumanMessage(content=\"whats my name?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Qm8xgHZQrEaF",
        "outputId": "595e66b6-2afd-4b51-9ec5-855a03ec8863"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"Based on our conversation, you haven't provided me with your name yet. If you'd like to share it with me, I'll be happy to remember it.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As expected, the first message where we stated our name has been trimmed. Plus there's now two new messages in the chat history (our latest question and the latest response). This means that even more information that used to be accessible in our conversation history is no longer available! In this case our initial math question has been trimmed from the history as well, so the model no longer knows about it:"
      ],
      "metadata": {
        "id": "FXMcpi6HX2VD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\n",
        "        \"messages\": [HumanMessage(content=\"what math problem did i ask?\")],\n",
        "        \"language\": \"English\",\n",
        "    },\n",
        "    config=config,\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "id": "Y4kbch7_rf53",
        "outputId": "13918d1f-9060-41b1-c0ac-467588f88a9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm sorry for any confusion, but it appears you haven't asked a math problem yet. I'm here to help, so please feel free to ask any math question or any other question you have, and I'll do my best to provide a helpful response.\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 105
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Streaming"
      ],
      "metadata": {
        "id": "CT4o49FFtMdS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we've got a functioning chatbot. However, one really important user experience (UX) consideration for chatbot application is streaming. LLMs can sometimes take a while to respond, and so in order to improve the user experience one thing that most applications do is stream back each token as it is generated. This allows the user to see progress.\n",
        "\n",
        "It's actually super easy to do this! All chains expose a `stream` method, and ones that use message history are no different. We can simply use that method to get back a streaming response:"
      ],
      "metadata": {
        "id": "w4cYu7wUYCk6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = {\"configurable\": {\"session_id\": \"abc15\"}}\n",
        "\n",
        "for r in with_message_history.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"hi! I'm todd. tell me a joke\")], \"language\": \"English\"},\n",
        "    config=config,\n",
        "):\n",
        "    print(r.content, end=\"|\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gJtZ0_YytWl5",
        "outputId": "8e607960-b703-4a60-e9e0-0de28e210b88"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|Hello| Todd|!| Sure|,| here|'|s| a| joke| for| you|:|\n",
            "\n",
            "Why| don|'|t| scientists| trust| atoms|?|\n",
            "\n",
            "Because| they| make| up| everything|!||"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Conclusion"
      ],
      "metadata": {
        "id": "dV3movEgZF43"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tutorial you've created your first simple chatbot. You've learned how to manage conversation history for multiple users and you've applied streaming. Now that you understand the basics of how to create a chatbot in LangChain, you are ready for more advanced topics! Check the [LangChain tutorials](https://python.langchain.com/v0.2/docs/tutorials/) for some more in-depth tutorials!"
      ],
      "metadata": {
        "id": "pLfhOIXwYvOn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "## Sources\n",
        "\n",
        "- [LangChain on Wikipedia](https://en.wikipedia.org/wiki/LangChain)\n",
        "- [Prompt engineering on Wikipedia](https://en.wikipedia.org/wiki/Prompt_engineering)\n",
        "- [LangChain Basics Tutorial #1 - LLMs & PromptTemplates with Colab](https://youtu.be/J_0qvRt4LNk?si=DWqG9Jvifqbhy-uu) by Sam Witteveen (2023), YouTube video.\n",
        "- [Langchain (Agents, Tools, Chains & Memory) for utilizing the full potential of LLMs](https://medium.com/@saumitra1joshi/langchain-agents-tools-chains-memory-for-utilizing-the-full-potential-of-llms-211e5dfee3fa) by Saumitra Joshi (2023), Medium article.\n",
        "- [Generative AI with LangChain](https://www.packtpub.com/en-us/product/generative-ai-with-langchain-9781835083468) by Ben Auffrath (2023), textbook published by Packt Publishing.\n",
        "- [LangChain v0.2 documentation](https://python.langchain.com/v0.2/docs/introduction/)"
      ],
      "metadata": {
        "id": "ozcXX56ByL--"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "fXetoMQFMnc7"
      }
    }
  ]
}